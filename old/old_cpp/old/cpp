
// ===== VERSION PARALLÈLE ULTRA-OPTIMISÉE =====

struct NodeDataOptimized {
    std::vector<double> data;
    size_t n;
    
    explicit NodeDataOptimized(pybind11::array_t<double> arr) {
        ArrayInfo info(arr);
        n = info.n;
        data.resize(n * 2);
        
        // Copie vectorisée
        for (size_t i = 0; i < n; ++i) {
            const double* src = info.get_point(i);
            data[i * 2] = src[0];
            data[i * 2 + 1] = src[1];
        }
    }
    
    inline double* get_point(size_t i) { return &data[i * 2]; }
    inline const double* get_point(size_t i) const { return &data[i * 2]; }
    
    pybind11::array_t<double> to_array() const {
        auto result = pybind11::array_t<double>(std::vector<ptrdiff_t>{static_cast<ptrdiff_t>(n), 2});
        ArrayInfo result_info(result);
        
        for (size_t i = 0; i < n; ++i) {
            double* dst = result_info.get_point(i);
            const double* src = get_point(i);
            dst[0] = src[0];
            dst[1] = src[1];
        }
        return result;
    }
};

// Versions natives ultra-rapides
double cout_graph_p2_native(const NodeDataOptimized& nodes, const NodeDataOptimized& targets) {
    double cout = 0;
    for (size_t i = 0; i < nodes.n; ++i) {
        const double* node_ptr = nodes.get_point(i);
        const double* target_ptr = targets.get_point(i);
        cout += distance_squared(node_ptr, target_ptr);
    }
    return std::sqrt(cout);
}

bool is_graph_connected_bfs_native(const NodeDataOptimized& nodes, double dist_threshold) {
    if (nodes.n <= 1) return true;
    
    const double threshold_sq = dist_threshold * dist_threshold;
    std::vector<bool> visited(nodes.n, false);
    std::deque<size_t> queue;
    
    queue.push_back(0);
    visited[0] = true;
    size_t visited_count = 1;

    while (!queue.empty() && visited_count < nodes.n) {
        size_t current = queue.front();
        queue.pop_front();
        
        const double* current_ptr = nodes.get_point(current);
        
        for (size_t i = 0; i < nodes.n; ++i) {
            if (!visited[i]) {
                const double* other_ptr = nodes.get_point(i);
                if (distance_squared(current_ptr, other_ptr) < threshold_sq) {
                    visited[i] = true;
                    queue.push_back(i);
                    ++visited_count;
                }
            }
        }
    }
    
    return visited_count == nodes.n;
}

NodeDataOptimized mutate_nodes_native(const NodeDataOptimized& nodes, double stepsize) {
    NodeDataOptimized result = nodes;
    
    static thread_local std::mt19937 gen(std::random_device{}());
    std::normal_distribution<double> dist(0.0, stepsize);
    
    for (size_t i = 0; i < nodes.n * 2; ++i) {
        result.data[i] += dist(gen);
    }
    
    return result;
}

std::vector<NodeDataOptimized> optimize_nodes_history_native(
    NodeDataOptimized nodes, 
    const NodeDataOptimized& targets, 
    double dist_threshold, 
    double stepsize, 
    size_t n
) {
    std::vector<NodeDataOptimized> history;
    history.reserve(n / 100 + 1);
    
    double best_error = cout_graph_p2_native(nodes, targets);
    history.push_back(nodes);
    
    size_t consecutive_failures = 0;
    const size_t max_failures = n / 10;
    
    for (size_t i = 0; i < n; ++i) {
        double progress = static_cast<double>(i) / n;
        double current_stepsize = stepsize * std::exp(-2.0 * progress);
        
        NodeDataOptimized new_nodes = mutate_nodes_native(nodes, current_stepsize);
        double new_error = cout_graph_p2_native(new_nodes, targets);
        
        if (new_error < best_error) {
            if (is_graph_connected_bfs_native(new_nodes, dist_threshold)) {
                nodes = std::move(new_nodes);
                best_error = new_error;
                consecutive_failures = 0;
                
                if (i % (n / 100 + 1) == 0 || i == n - 1) {
                    history.push_back(nodes);
                }
            } else {
                ++consecutive_failures;
            }
        } else {
            ++consecutive_failures;
        }
        
        if (consecutive_failures > max_failures) {
            break;
        }
    }
    
    return history;
}

std::vector<std::vector<pybind11::array_t<double>>> optimize_nodes_history_parallel(
    pybind11::array_t<double> nodes,
    pybind11::array_t<double> targets,
    double dist_threshold,
    double stepsize,
    size_t n,
    size_t n_threads
) {
    // Conversion avant parallélisation
    NodeDataOptimized nodes_native(nodes);
    NodeDataOptimized targets_native(targets);
    
    std::vector<std::vector<NodeDataOptimized>> all_histories_native(n_threads);
    std::vector<std::thread> threads;
    threads.reserve(n_threads);

    // Relâcher le GIL pour le parallélisme pur
    pybind11::gil_scoped_release release;

    auto worker = [&](size_t thread_id) {
        try {
            pybind11::gil_scoped_acquire acquire;
            auto history = optimize_nodes_history_native(
                nodes_native, targets_native, dist_threshold, stepsize, n
            );
            all_histories_native[thread_id] = std::move(history);
        } catch (const std::exception& e) {
            std::cerr << "Exception in thread " << thread_id << ": " << e.what() << std::endl;
        }
    };

    for (size_t i = 0; i < n_threads; ++i) {
        threads.emplace_back(worker, i);
    }
    
    for (auto& t : threads) {
        t.join();
    }

    // Reconversion efficace
    std::vector<std::vector<pybind11::array_t<double>>> all_histories(n_threads);
    for (size_t i = 0; i < n_threads; ++i) {
        all_histories[i].reserve(all_histories_native[i].size());
        for (const auto& node_data : all_histories_native[i]) {
            all_histories[i].push_back(node_data.to_array());
        }
    }
    
    return all_histories;
}







// === 2 ===


// ===== Version Claude 4 =====

#include <queue>           // MANQUANT
#include <condition_variable>  // MANQUANT
#include <future>          // MANQUANT
#include <atomic>          // MANQUANT
#include <functional>      // MANQUANT
#include <memory>          // MANQUANT


// ===== OPTIMISATIONS PROPOSÉES =====


// Pool de threads réutilisable
class ThreadPool {
private:
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex queue_mutex;
    std::condition_variable condition;
    bool stop;

public:
    ThreadPool(size_t threads) : stop(false) {
        for(size_t i = 0; i < threads; ++i) {
            workers.emplace_back([this] {
                for(;;) {
                    std::function<void()> task;
                    {
                        std::unique_lock<std::mutex> lock(this->queue_mutex);
                        this->condition.wait(lock, [this]{ return this->stop || !this->tasks.empty(); });
                        if(this->stop && this->tasks.empty()) return;
                        task = std::move(this->tasks.front());
                        this->tasks.pop();
                    }
                    task();
                }
            });
        }
    }

    template<class F>
    void enqueue(F&& f) {
        {
            std::unique_lock<std::mutex> lock(queue_mutex);
            if(stop) throw std::runtime_error("enqueue on stopped ThreadPool");
            tasks.emplace(std::forward<F>(f));
        }
        condition.notify_one();
    }

    size_t size() const { return workers.size(); }

    ~ThreadPool() {
        {
            std::unique_lock<std::mutex> lock(queue_mutex);
            stop = true;
        }
        condition.notify_all();
        for(std::thread &worker: workers) worker.join();
    }
};

// Version optimisée de optimize_nodes avec réutilisation de buffers
pybind11::array_t<double> optimize_nodes_optimized(
    pybind11::array_t<double> nodes, 
    pybind11::array_t<double> targets, 
    double dist_threshold, 
    double stepsize, 
    size_t n,
    std::mt19937& gen
) {
    auto current_nodes = nodes;
    double best_error = cout_graph_p2(current_nodes, targets);
    
    const size_t max_failures = std::max(size_t(10), n / 20);
    size_t consecutive_failures = 0;
    
    // Réutiliser le même buffer pour les mutations
    auto mutation_buffer = pybind11::array_t<double>(std::vector<ptrdiff_t>{
        static_cast<ptrdiff_t>(ArrayInfo(nodes).n), 2});
    
    std::normal_distribution<double> dist(0.0, 1.0);
    
    for (size_t i = 0; i < n; ++i) {
        double progress = static_cast<double>(i) / n;
        double current_stepsize = stepsize * std::exp(-3.0 * progress * progress);
        
        // Mutation in-place dans le buffer
        ArrayInfo current_info(current_nodes);
        ArrayInfo buffer_info(mutation_buffer);
        
        for (size_t j = 0; j < current_info.n; ++j) {
            const double* src = current_info.get_point(j);
            double* dst = buffer_info.get_point(j);
            dst[0] = src[0] + current_stepsize * dist(gen);
            dst[1] = src[1] + current_stepsize * dist(gen);
        }
        
        double new_error = cout_graph_p2(mutation_buffer, targets);
        
        if (new_error < best_error) {
            if (is_graph_connected_bfs(mutation_buffer, dist_threshold)) {
                // Copy seulement si nécessaire
                std::memcpy(current_info.get_point(0), buffer_info.get_point(0), 
                           current_info.n * 2 * sizeof(double));
                best_error = new_error;
                consecutive_failures = 0;
            } else {
                ++consecutive_failures;
            }
        } else {
            ++consecutive_failures;
        }
        
        if (consecutive_failures > max_failures) {
            break;
        }
        
        // Adaptation dynamique du stepsize
        if (consecutive_failures > max_failures / 2) {
            stepsize *= 0.95;
        }
    }
    
    return current_nodes;
}

// Version parallèle ultra-optimisée (version simple sans pool de threads)
std::vector<pybind11::array_t<double>> optimize_nodes_parallel_v2(
    pybind11::array_t<double> nodes,
    pybind11::array_t<double> targets,
    double dist_threshold,
    double stepsize,
    size_t n,
    size_t n_threads
) {
    std::vector<pybind11::array_t<double>> results(n_threads);
    std::vector<std::thread> threads;
    
    // Pré-allocation des générateurs pour éviter contentions
    std::vector<std::mt19937> generators(n_threads);
    std::random_device rd;
    for (size_t i = 0; i < n_threads; ++i) {
        generators[i].seed(rd() + i);
    }
    
    pybind11::gil_scoped_release release;
    
    auto worker = [&](size_t thread_id) {
        try {
            pybind11::gil_scoped_acquire acquire;
            results[thread_id] = optimize_nodes_optimized(nodes, targets, dist_threshold, stepsize, n, generators[thread_id]);
        } catch (const std::exception& e) {
            std::cout << "Exception in thread " << thread_id << ": " << e.what() << std::endl;
        }
    };
    
    for (size_t i = 0; i < n_threads; ++i) {
        threads.emplace_back(worker, i);
    }
    
    for (auto& t : threads) {
        t.join();
    }
    
    return results;
}

// Version avec work-stealing pour meilleur load balancing
std::vector<pybind11::array_t<double>> optimize_nodes_parallel_workstealing(
    pybind11::array_t<double> nodes,
    pybind11::array_t<double> targets,
    double dist_threshold,
    double stepsize,
    size_t total_iterations,
    size_t n_threads
) {
    std::vector<pybind11::array_t<double>> results(n_threads);
    std::atomic<size_t> work_counter(0);
    std::vector<std::thread> threads;
    
    const size_t chunk_size = std::max(size_t(100), total_iterations / (n_threads * 4));
    
    pybind11::gil_scoped_release release;
    
    auto worker = [&](size_t thread_id) {
        try {
            pybind11::gil_scoped_acquire acquire;
            
            std::mt19937 gen(std::random_device{}() + thread_id);
            auto best_nodes = nodes;
            double best_error = cout_graph_p2(best_nodes, targets);
            
            size_t local_iterations;
            while ((local_iterations = work_counter.fetch_add(chunk_size)) < total_iterations) {
                size_t end_iter = std::min(local_iterations + chunk_size, total_iterations);
                
                for (size_t i = local_iterations; i < end_iter; ++i) {
                    double progress = static_cast<double>(i) / total_iterations;
                    double current_stepsize = stepsize * std::exp(-2.0 * progress);
                    
                    auto new_nodes = mutate_nodes(best_nodes, current_stepsize);
                    double new_error = cout_graph_p2(new_nodes, targets);
                    
                    if (new_error < best_error && 
                        is_graph_connected_bfs(new_nodes, dist_threshold)) {
                        best_nodes = std::move(new_nodes);
                        best_error = new_error;
                    }
                }
            }
            
            results[thread_id] = best_nodes;
        } catch (const std::exception& e) {
            std::cout << "Exception in thread " << thread_id << ": " << e.what() << std::endl;
        }
    };
    
    for (size_t i = 0; i < n_threads; ++i) {
        threads.emplace_back(worker, i);
    }
    
    for (auto& t : threads) {
        t.join();
    }
    
    return results;
}

// Version hybride avec partage de meilleurs résultats
std::vector<pybind11::array_t<double>> optimize_nodes_parallel_hybrid(
    pybind11::array_t<double> nodes,
    pybind11::array_t<double> targets,
    double dist_threshold,
    double stepsize,
    size_t n,
    size_t n_threads
) {
    std::vector<pybind11::array_t<double>> results(n_threads);
    std::vector<std::thread> threads;
    
    // Partage périodique des meilleurs résultats
    std::mutex best_mutex;
    pybind11::array_t<double> global_best = nodes;
    double global_best_error = cout_graph_p2(nodes, targets);
    
    const size_t sync_interval = n / 10;
    
    pybind11::gil_scoped_release release;
    
    auto worker = [&](size_t thread_id) {
        try {
            pybind11::gil_scoped_acquire acquire;
            
            std::mt19937 gen(std::random_device{}() + thread_id);
            auto local_best = nodes;
            double local_best_error = cout_graph_p2(local_best, targets);
            
            for (size_t i = 0; i < n; ++i) {
                // Synchronisation périodique
                if (i % sync_interval == 0 && i > 0) {
                    std::lock_guard<std::mutex> lock(best_mutex);
                    if (global_best_error < local_best_error) {
                        local_best = global_best;
                        local_best_error = global_best_error;
                    } else if (local_best_error < global_best_error) {
                        global_best = local_best;
                        global_best_error = local_best_error;
                    }
                }
                
                double progress = static_cast<double>(i) / n;
                double current_stepsize = stepsize * std::exp(-2.0 * progress);
                
                auto new_nodes = mutate_nodes(local_best, current_stepsize);
                double new_error = cout_graph_p2(new_nodes, targets);
                
                if (new_error < local_best_error && 
                    is_graph_connected_bfs(new_nodes, dist_threshold)) {
                    local_best = std::move(new_nodes);
                    local_best_error = new_error;
                }
            }
            
            results[thread_id] = local_best;
        } catch (const std::exception& e) {
            std::cout << "Exception in thread " << thread_id << ": " << e.what() << std::endl;
        }
    };
    
    for (size_t i = 0; i < n_threads; ++i) {
        threads.emplace_back(worker, i);
    }
    
    for (auto& t : threads) {
        t.join();
    }
    
    return results;
}






// pybind11::array_t<double> optimize_nodes(
//     pybind11::array_t<double> nodes, 
//     pybind11::array_t<double> targets, 
//     double dist_threshold, 
//     double stepsize, 
//     size_t n
// ) {
//     auto current_nodes = nodes;
//     double best_error = cout_graph_p2(current_nodes, targets);
//     size_t consecutive_failures = 0;
//     const size_t max_failures = n / 10; // Adaptative early stopping
    
//     for (size_t i = 0; i < n; ++i) {
//         // Cooling schedule non-linéaire plus efficace
//         double progress = static_cast<double>(i) / n;
//         double current_stepsize = stepsize * std::exp(-2.0 * progress); // Décroissance exponentielle
        
//         auto new_nodes = mutate_nodes(current_nodes, current_stepsize);
//         double new_error = cout_graph_p2(new_nodes, targets);
        
//         if (new_error < best_error) {
//             if (is_graph_connected_bfs(new_nodes, dist_threshold)) {
//                 current_nodes = std::move(new_nodes);
//                 best_error = new_error;
//                 consecutive_failures = 0;
//             } else {
//                 ++consecutive_failures;
//             }
//         } else {
//             ++consecutive_failures;
//         }
        
//         // Early stopping si trop d'échecs consécutifs
//         if (consecutive_failures > max_failures) {
//             break;
//         }
//     }
    
//     return current_nodes;
// }